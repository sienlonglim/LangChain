{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6e49bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time\n",
    "from openai import OpenAI\n",
    "from io import BytesIO\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the API key from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI() # The default API will call os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# This if we want to use the .env file's API key\n",
    "#import dotenv\n",
    "#client = OpenAI(api_key=dotenv.get_key(\".env\", 'OPENAI_API_KEY')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic prompting\n",
    "Available models:\n",
    "1. gpt-3.5-turbo (maximum limit 4097 tokens)\n",
    "2. dall-e-3\n",
    "3. whisper-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletionUsage(completion_tokens=174, prompt_tokens=30, total_tokens=204)\n",
      "\n",
      "  {\n",
      "    \"OpenAI Migrate\": {\n",
      "      \"Description\": \"OpenAI Migrate is a tool designed to help users transition from older versions of OpenAI's API to newer versions. It provides guidance and automated suggestions to update code and configurations for compatibility with the latest OpenAI API.\",\n",
      "      \"Features\": [\n",
      "        \"Automated code and configuration updates\",\n",
      "        \"Compatibility checks for older code\",\n",
      "        \"Guidance on API changes and best practices\"\n",
      "      ],\n",
      "      \"Usage\": {\n",
      "        \"1\": \"Users can provide their existing code or configuration files to Migrate for analysis\",\n",
      "        \"2\": \"Migrate will then identify areas that need updates and provide suggestions for making the necessary changes\",\n",
      "        \"3\": \"Users can review the suggestions and apply them to their code before transitioning to the latest OpenAI API\"\n",
      "      }\n",
      "    }\n",
      "  }\n"
     ]
    }
   ],
   "source": [
    "# JSON format\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model = \"gpt-3.5-turbo-1106\",\n",
    "    response_format={ \"type\": \"json_object\" },\n",
    "    seed = None, # Optional, allows more deterministic response\n",
    "    temperature = 1, # Default 1, between 0-2, higher temperatures = more random\n",
    "    messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain to me how openai migrate works\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(response.usage)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-response streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00s : 1\n",
      "0.00s : \n",
      "\n",
      "0.00s : 2\n",
      "0.01s : \n",
      "\n",
      "0.01s : 3\n",
      "0.01s : \n",
      "\n",
      "0.01s : 4\n",
      "0.01s : \n",
      "\n",
      "0.01s : 5\n",
      "0.02s : \n",
      "\n",
      "0.02s : 6\n",
      "0.02s : \n",
      "\n",
      "0.02s : 7\n",
      "0.02s : \n",
      "\n",
      "0.02s : 8\n",
      "0.03s : \n",
      "\n",
      "0.03s : 9\n",
      "0.03s : \n",
      "\n",
      "0.03s : 10\n",
      "Full response received 0.03 seconds after request\n",
      "Full conversation received: 1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model = \"gpt-3.5-turbo-1106\",\n",
    "    stream = True, # event stream like ChatGPT\n",
    "    seed = None, # Optional, allows more deterministic response\n",
    "    temperature = 1, # Default 1, between 0-2, higher temperatures = more random\n",
    "    messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Count from 1 up to 10, answer line by line\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "# create variables to collect the stream of chunks\n",
    "collected_chunks = []\n",
    "collected_messages = []\n",
    "# iterate through the stream of events\n",
    "for chunk in response:\n",
    "    chunk_time = time.time() - start_time  # calculate the time delay of the chunk\n",
    "    collected_chunks.append(chunk)  # save the event response\n",
    "    chunk_message = chunk.choices[0].delta.content\n",
    "    if chunk_message:\n",
    "      collected_messages.append(chunk_message)  # save the message\n",
    "      print(f\"{chunk_time:.2f}s : {chunk_message}\")  # print the delay and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full response received 0.03 seconds after request\n",
      "Full conversation received: \n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# print the time delay and text received\n",
    "print(f\"Full response received {chunk_time:.2f} seconds after request\")\n",
    "full_reply_content = ''.join(collected_messages)\n",
    "print(f\"Full conversation received: \\n{full_reply_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use LLM to refine basic prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletionUsage(completion_tokens=97, prompt_tokens=107, total_tokens=204)\n",
      "Generate an image of a charming Maltipoo with short legs and a slender build. The Maltipoo has a frontal view, with short, curly ivory hair on its head and frizzy hair on its body. It is depicted in the living room of a stylish Singapore apartment, illuminated by abundant sunlight. The apartment boasts white parquet flooring, creating a bright and welcoming atmosphere. The adorable Maltipoo is captured smiling at its owner, exuding joy and affection.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model = \"gpt-3.5-turbo-1106\",\n",
    "    seed = None, # Optional, allows more deterministic response\n",
    "    temperature = 1, # Default 1, between 0-2, higher temperatures = more random\n",
    "    messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": '''Refine this prompt to be used in dall-e-3 image generation, add as many descriptive terms as possible: \n",
    "     Frontal view of a short-legged and slim maltipoo, \n",
    "    it has short curly ivory hair at its head, \n",
    "    frizzy hair on its body. It is in living room of a Singapore apartment on a sunny day, \n",
    "    the apartment has white parquet flooring, smiling at the sight of its owner'''}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(response.usage)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BadRequestError' object has no attribute 'http_status'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mf:\\python_stuff\\LangChain\\OpenAI_APIs.ipynb Cell 12\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/python_stuff/LangChain/OpenAI_APIs.ipynb#X14sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/python_stuff/LangChain/OpenAI_APIs.ipynb#X14sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m   response \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mimages\u001b[39m.\u001b[39;49mgenerate(\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/python_stuff/LangChain/OpenAI_APIs.ipynb#X14sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdall-e-3\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/python_stuff/LangChain/OpenAI_APIs.ipynb#X14sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     prompt\u001b[39m=\u001b[39;49mfinal_prompt,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/python_stuff/LangChain/OpenAI_APIs.ipynb#X14sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     size\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m1024x1024\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/python_stuff/LangChain/OpenAI_APIs.ipynb#X14sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     quality\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mhd\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m#standard or hd\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/python_stuff/LangChain/OpenAI_APIs.ipynb#X14sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     style \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mvivid\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m#vivid or natural\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/python_stuff/LangChain/OpenAI_APIs.ipynb#X14sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     n\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/python_stuff/LangChain/OpenAI_APIs.ipynb#X14sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m   )\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/python_stuff/LangChain/OpenAI_APIs.ipynb#X14sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRevised prompt: \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mdata[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mrevised_prompt\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mf:\\python_stuff\\LangChain\\.venv\\lib\\site-packages\\openai\\resources\\images.py:251\u001b[0m, in \u001b[0;36mImages.generate\u001b[1;34m(self, prompt, model, n, quality, response_format, size, style, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[39mCreates an image given a prompt.\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[39m  timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[0;32m    252\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m/images/generations\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    253\u001b[0m     body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[0;32m    254\u001b[0m         {\n\u001b[0;32m    255\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mprompt\u001b[39;49m\u001b[39m\"\u001b[39;49m: prompt,\n\u001b[0;32m    256\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[0;32m    257\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[0;32m    258\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mquality\u001b[39;49m\u001b[39m\"\u001b[39;49m: quality,\n\u001b[0;32m    259\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[0;32m    260\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39msize\u001b[39;49m\u001b[39m\"\u001b[39;49m: size,\n\u001b[0;32m    261\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mstyle\u001b[39;49m\u001b[39m\"\u001b[39;49m: style,\n\u001b[0;32m    262\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[0;32m    263\u001b[0m         },\n\u001b[0;32m    264\u001b[0m         image_generate_params\u001b[39m.\u001b[39;49mImageGenerateParams,\n\u001b[0;32m    265\u001b[0m     ),\n\u001b[0;32m    266\u001b[0m     options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[0;32m    267\u001b[0m         extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[0;32m    268\u001b[0m     ),\n\u001b[0;32m    269\u001b[0m     cast_to\u001b[39m=\u001b[39;49mImagesResponse,\n\u001b[0;32m    270\u001b[0m )\n",
      "File \u001b[1;32mf:\\python_stuff\\LangChain\\.venv\\lib\\site-packages\\openai\\_base_client.py:1063\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1060\u001b[0m opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[0;32m   1061\u001b[0m     method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[0;32m   1062\u001b[0m )\n\u001b[1;32m-> 1063\u001b[0m \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[1;32mf:\\python_stuff\\LangChain\\.venv\\lib\\site-packages\\openai\\_base_client.py:842\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    833\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    834\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    835\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    840\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    841\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m--> 842\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[0;32m    843\u001b[0m         cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[0;32m    844\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[0;32m    845\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    846\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[0;32m    847\u001b[0m         remaining_retries\u001b[39m=\u001b[39;49mremaining_retries,\n\u001b[0;32m    848\u001b[0m     )\n",
      "File \u001b[1;32mf:\\python_stuff\\LangChain\\.venv\\lib\\site-packages\\openai\\_base_client.py:885\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    884\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n\u001b[1;32m--> 885\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    886\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mTimeoutException \u001b[39mas\u001b[39;00m err:\n",
      "\u001b[1;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'code': 'content_policy_violation', 'message': 'Your request was rejected as a result of our safety system. Image descriptions generated from your prompt may contain text that is not allowed by our safety system. If you believe this was done in error, your request may succeed if retried, or by adjusting your prompt.', 'param': None, 'type': 'invalid_request_error'}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mf:\\python_stuff\\LangChain\\OpenAI_APIs.ipynb Cell 12\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/python_stuff/LangChain/OpenAI_APIs.ipynb#X14sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m   \u001b[39mprint\u001b[39m(image_url)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/python_stuff/LangChain/OpenAI_APIs.ipynb#X14sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mexcept\u001b[39;00m openai\u001b[39m.\u001b[39mOpenAIError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/python_stuff/LangChain/OpenAI_APIs.ipynb#X14sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m   \u001b[39mprint\u001b[39m(e\u001b[39m.\u001b[39;49mhttp_status)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/python_stuff/LangChain/OpenAI_APIs.ipynb#X14sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m   \u001b[39mprint\u001b[39m(e\u001b[39m.\u001b[39merror)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BadRequestError' object has no attribute 'http_status'"
     ]
    }
   ],
   "source": [
    "maltipoo_prompt = '''Frontal view of a short-legged and slim maltipoo, \\\n",
    "it has short curly ivory hair at its head, \\\n",
    "frizzy hair on its body. It is in living room of a Singapore apartment on a sunny day, \\\n",
    "the apartment has white parquet flooring, smiling at the sight of its owner'''\n",
    "\n",
    "human_prompt = '''Full body studio shoot of a young king in armour'''\n",
    "\n",
    "art_prompt = '''Abstract, surreal landscape, musical notes, sun rays'''\n",
    "\n",
    "instructional_prompt = '''\n",
    "Generate images, but in a very specific way. Use the exact prompt that is provided without any modifications or rewrites.\\n\\\n",
    "Here is what I mean.\\n\\\n",
    "This is an example of an exact prompt using comma list: \\n\\\n",
    "\"A T-Rex with sunglasses on a bicycle, Art Nouveau, High Fashion, 4k, hyper detailed, wide aspect ratio\"\\n\\\n",
    "Normally you'll rewrite it in something like this and generate the image based on the second prompt: \\n\\\n",
    "\"A wide 4k hyper-detailed Art Nouveau style illustration showcasing a T-Rex wearing sunglasses, riding a bicycle, all imbued with a touch of high fashion.\"\\n\\\n",
    "I don't want the rewrite. I need an image generated by the exact prompt. I know you can do it.\\n\\\n",
    "The first two prompts were just explanation examples.\\n\\\n",
    "This is the prompt to work with: \\n'''\n",
    "\n",
    "exact_prompt = ''\n",
    "\n",
    "final_prompt = instructional_prompt + human_prompt\n",
    "try:\n",
    "  response = client.images.generate(\n",
    "    model=\"dall-e-3\",\n",
    "    prompt=final_prompt,\n",
    "    size=\"1024x1024\",\n",
    "    quality=\"hd\", #standard or hd\n",
    "    style = 'vivid', #vivid or natural\n",
    "    n=1\n",
    "  )\n",
    "\n",
    "  print(f\"Revised prompt: {response.data[0].revised_prompt}\")\n",
    "  image_url = response.data[0].url\n",
    "  print(image_url)\n",
    "\n",
    "except openai.OpenAIError as e:\n",
    "  print(e.http_status)\n",
    "  print(e.error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking\n",
    "Provide:\n",
    "1. image (original image)\n",
    "2. mask (transparent areas as where editing to be done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.images.edit(\n",
    "  model=\"dall-e-2\",\n",
    "  image=open(\"ignore/sunlit_lounge.png\", \"rb\"),\n",
    "  mask=open(\"ignore/mask.png\", \"rb\"),\n",
    "  prompt=\"A sunlit indoor lounge area with a pool containing a flamingo\",\n",
    "  n=1,\n",
    "  size=\"1024x1024\"\n",
    ")\n",
    "image_url = response.data[0].url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the image file from disk and resize it\n",
    "filepath = 'ignore/images/futuristiclink.png'\n",
    "image = Image.open(filepath)\n",
    "width, height = 256, 256\n",
    "image = image.resize((width, height))\n",
    "\n",
    "# Convert the image to a BytesIO object\n",
    "byte_stream = BytesIO()\n",
    "image.save(byte_stream, format='PNG')\n",
    "byte_array = byte_stream.getvalue()\n",
    "\n",
    "response = client.images.create_variation(\n",
    "  image=byte_array,\n",
    "  n=1,\n",
    "  size=\"1024x1024\"\n",
    ")\n",
    "\n",
    "for i in response.data:\n",
    "    print(i.url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
