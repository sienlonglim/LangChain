{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6e49bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time\n",
    "from openai import OpenAI\n",
    "from io import BytesIO\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the API key from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI() # The default API will call os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# This if we want to use the .env file's API key\n",
    "#import dotenv\n",
    "#client = OpenAI(api_key=dotenv.get_key(\".env\", 'OPENAI_API_KEY')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic prompting\n",
    "Available models:\n",
    "1. gpt-3.5-turbo (maximum limit 4097 tokens)\n",
    "2. dall-e-3\n",
    "3. whisper-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletionUsage(completion_tokens=174, prompt_tokens=30, total_tokens=204)\n",
      "\n",
      "  {\n",
      "    \"OpenAI Migrate\": {\n",
      "      \"Description\": \"OpenAI Migrate is a tool designed to help users transition from older versions of OpenAI's API to newer versions. It provides guidance and automated suggestions to update code and configurations for compatibility with the latest OpenAI API.\",\n",
      "      \"Features\": [\n",
      "        \"Automated code and configuration updates\",\n",
      "        \"Compatibility checks for older code\",\n",
      "        \"Guidance on API changes and best practices\"\n",
      "      ],\n",
      "      \"Usage\": {\n",
      "        \"1\": \"Users can provide their existing code or configuration files to Migrate for analysis\",\n",
      "        \"2\": \"Migrate will then identify areas that need updates and provide suggestions for making the necessary changes\",\n",
      "        \"3\": \"Users can review the suggestions and apply them to their code before transitioning to the latest OpenAI API\"\n",
      "      }\n",
      "    }\n",
      "  }\n"
     ]
    }
   ],
   "source": [
    "# JSON format\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model = \"gpt-3.5-turbo-1106\",\n",
    "    response_format={ \"type\": \"json_object\" },\n",
    "    seed = None, # Optional, allows more deterministic response\n",
    "    temperature = 1, # Default 1, between 0-2, higher temperatures = more random\n",
    "    messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain to me how openai migrate works\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(response.usage)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-response streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00s : 1\n",
      "0.00s : \n",
      "\n",
      "0.00s : 2\n",
      "0.01s : \n",
      "\n",
      "0.01s : 3\n",
      "0.01s : \n",
      "\n",
      "0.01s : 4\n",
      "0.01s : \n",
      "\n",
      "0.01s : 5\n",
      "0.02s : \n",
      "\n",
      "0.02s : 6\n",
      "0.02s : \n",
      "\n",
      "0.02s : 7\n",
      "0.02s : \n",
      "\n",
      "0.02s : 8\n",
      "0.03s : \n",
      "\n",
      "0.03s : 9\n",
      "0.03s : \n",
      "\n",
      "0.03s : 10\n",
      "Full response received 0.03 seconds after request\n",
      "Full conversation received: 1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model = \"gpt-3.5-turbo-1106\",\n",
    "    stream = True, # event stream like ChatGPT\n",
    "    seed = None, # Optional, allows more deterministic response\n",
    "    temperature = 1, # Default 1, between 0-2, higher temperatures = more random\n",
    "    messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Count from 1 up to 10, answer line by line\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "# create variables to collect the stream of chunks\n",
    "collected_chunks = []\n",
    "collected_messages = []\n",
    "# iterate through the stream of events\n",
    "for chunk in response:\n",
    "    chunk_time = time.time() - start_time  # calculate the time delay of the chunk\n",
    "    collected_chunks.append(chunk)  # save the event response\n",
    "    chunk_message = chunk.choices[0].delta.content\n",
    "    if chunk_message:\n",
    "      collected_messages.append(chunk_message)  # save the message\n",
    "      print(f\"{chunk_time:.2f}s : {chunk_message}\")  # print the delay and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full response received 0.03 seconds after request\n",
      "Full conversation received: \n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# print the time delay and text received\n",
    "print(f\"Full response received {chunk_time:.2f} seconds after request\")\n",
    "full_reply_content = ''.join(collected_messages)\n",
    "print(f\"Full conversation received: \\n{full_reply_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use LLM to refine basic prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletionUsage(completion_tokens=97, prompt_tokens=107, total_tokens=204)\n",
      "Generate an image of a charming Maltipoo with short legs and a slender build. The Maltipoo has a frontal view, with short, curly ivory hair on its head and frizzy hair on its body. It is depicted in the living room of a stylish Singapore apartment, illuminated by abundant sunlight. The apartment boasts white parquet flooring, creating a bright and welcoming atmosphere. The adorable Maltipoo is captured smiling at its owner, exuding joy and affection.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model = \"gpt-3.5-turbo-1106\",\n",
    "    seed = None, # Optional, allows more deterministic response\n",
    "    temperature = 1, # Default 1, between 0-2, higher temperatures = more random\n",
    "    messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": '''Refine this prompt to be used in dall-e-3 image generation, add as many descriptive terms as possible: \n",
    "     Frontal view of a short-legged and slim maltipoo, \n",
    "    it has short curly ivory hair at its head, \n",
    "    frizzy hair on its body. It is in living room of a Singapore apartment on a sunny day, \n",
    "    the apartment has white parquet flooring, smiling at the sight of its owner'''}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(response.usage)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maltipoo_prompt = '''Frontal view of a short-legged and slim maltipoo, \\\n",
    "it has short curly ivory hair at its head, \\\n",
    "frizzy hair on its body. It is in living room of a Singapore apartment on a sunny day, \\\n",
    "the apartment has white parquet flooring, smiling at the sight of its owner'''\n",
    "\n",
    "human_prompt = '''Full body studio shoot of a young king in armour'''\n",
    "\n",
    "art_prompt = '''Abstract, surreal landscape, musical notes, sun rays'''\n",
    "\n",
    "instructional_prompt = '''\n",
    "Generate images, but in a very specific way. Use the exact prompt that is provided without any modifications or rewrites.\\n\\\n",
    "Here is what I mean.\\n\\\n",
    "This is an example of an exact prompt using comma list: \\n\\\n",
    "\"A T-Rex with sunglasses on a bicycle, Art Nouveau, High Fashion, 4k, hyper detailed, wide aspect ratio\"\\n\\\n",
    "Normally you'll rewrite it in something like this and generate the image based on the second prompt: \\n\\\n",
    "\"A wide 4k hyper-detailed Art Nouveau style illustration showcasing a T-Rex wearing sunglasses, riding a bicycle, all imbued with a touch of high fashion.\"\\n\\\n",
    "I don't want the rewrite. I need an image generated by the exact prompt. I know you can do it.\\n\\\n",
    "The first two prompts were just explanation examples.\\n\\\n",
    "This is the prompt to work with: \\n'''\n",
    "\n",
    "exact_prompt = ''\n",
    "\n",
    "final_prompt = instructional_prompt + human_prompt\n",
    "try:\n",
    "  response = client.images.generate(\n",
    "    model=\"dall-e-3\",\n",
    "    prompt=final_prompt,\n",
    "    size=\"1024x1024\",\n",
    "    quality=\"hd\", #standard or hd\n",
    "    style = 'vivid', #vivid or natural\n",
    "    n=1\n",
    "  )\n",
    "\n",
    "  print(f\"Revised prompt: {response.data[0].revised_prompt}\")\n",
    "  image_url = response.data[0].url\n",
    "  print(image_url)\n",
    "\n",
    "except openai.OpenAIError as e:\n",
    "  print(e.http_status)\n",
    "  print(e.error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking\n",
    "Provide:\n",
    "1. image (original image)\n",
    "2. mask (transparent areas as where editing to be done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.images.edit(\n",
    "  model=\"dall-e-2\",\n",
    "  image=open(\"ignore/sunlit_lounge.png\", \"rb\"),\n",
    "  mask=open(\"ignore/mask.png\", \"rb\"),\n",
    "  prompt=\"A sunlit indoor lounge area with a pool containing a flamingo\",\n",
    "  n=1,\n",
    "  size=\"1024x1024\"\n",
    ")\n",
    "image_url = response.data[0].url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the image file from disk and resize it\n",
    "filepath = 'ignore/images/futuristiclink.png'\n",
    "image = Image.open(filepath)\n",
    "width, height = 256, 256\n",
    "image = image.resize((width, height))\n",
    "\n",
    "# Convert the image to a BytesIO object\n",
    "byte_stream = BytesIO()\n",
    "image.save(byte_stream, format='PNG')\n",
    "byte_array = byte_stream.getvalue()\n",
    "\n",
    "response = client.images.create_variation(\n",
    "  image=byte_array,\n",
    "  n=1,\n",
    "  size=\"1024x1024\"\n",
    ")\n",
    "\n",
    "for i in response.data:\n",
    "    print(i.url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
